{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3150, 3150)\n",
      "[[ 1.          0.23822756  0.15470847 -0.37089518  0.47787689]\n",
      " [ 0.23822756  1.          0.18521063  0.33028608  0.59236819]\n",
      " [ 0.15470847  0.18521063  1.          0.11545991 -0.12820421]\n",
      " [-0.37089518  0.33028608  0.11545991  1.          0.03646353]\n",
      " [ 0.47787689  0.59236819 -0.12820421  0.03646353  1.        ]]\n",
      "3134 3134 3134 3134\n",
      "                                                  logDays\n",
      "userID                                                   \n",
      "186543                              [0.14866091268830786]\n",
      "531244  [0.14504132891357085, 0.14919610396535354, 0.1...\n",
      "588117  [0.14723022240461198, 0.14750134522524172, 0.1...\n",
      "596178  [0.1446917436599719, 0.1446917436599719, 0.144...\n",
      "672696  [0.1456900885317203, 0.1456900885317203, 0.146...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import doc2vec\n",
    "import word2vec\n",
    "import rank_metrics\n",
    "import datetime\n",
    "import pickle\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn.metrics.pairwise import euclidean_distances, pairwise_distances\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from ast import literal_eval\n",
    "\n",
    "listOfAlgs = [ \"attributeCosineSim\", \"doc2vec\", \"word2vec\"]\n",
    "embedSizes = [32, 64, 128]\n",
    "windowSizes = [1, 3, 5]\n",
    "#SELECT userID, min(lastModified), max(lastModified) FROM `userEvents` group by userID having max(lastModified) >= \"2018-06-01\" and min(lastModified) < \"2018-06-01\"\n",
    "\n",
    "dfValidDates = pd.read_csv(\"data/serialValidDates.csv\", sep=\";\", header=0, index_col=0)\n",
    "dfValidDates.novelty_date = pd.to_datetime(dfValidDates.novelty_date)\n",
    "#now = datetime.datetime.now()\n",
    "now = datetime.datetime(2018, 7, 16)\n",
    "novelty_score = 1 / np.log((now - dfValidDates.novelty_date).dt.days + 2.72)\n",
    "#print(novelty_score)\n",
    "dfValidDates[\"noveltyScore\"] = novelty_score\n",
    "dct = defaultdict(int)\n",
    "noveltyDict = dfValidDates.noveltyScore.to_dict(into=dct)\n",
    "\n",
    "dfCBFeatures = pd.read_csv(\"data/serialCBFeatures.txt\", sep=\",\", header=0, index_col=0)\n",
    "dfCBSim = 1 - pairwise_distances(dfCBFeatures, metric=\"cosine\")\n",
    "\n",
    "#denote the\n",
    "dfCBSimNoSame = np.copy(dfCBSim)\n",
    "np.fill_diagonal(dfCBSimNoSame, 0.0)\n",
    "\n",
    "cbNames = dfCBFeatures.index.values\n",
    "cbVals = range(len(cbNames))\n",
    "rev_cbDict = dict(zip(cbVals, cbNames))\n",
    "cbDict = dict(zip(cbNames, cbVals))\n",
    "\n",
    "print(dfCBSim.shape)\n",
    "print(dfCBSim[0:5,0:5])\n",
    "\n",
    "df = pd.read_csv(\"data/serialTexts.txt\", sep=\";\", header=0, index_col=0)\n",
    "d2v_names = df.index.values\n",
    "d2v_vals = range(len(d2v_names))\n",
    "\n",
    "rev_dict_d2v = dict(zip(d2v_vals, d2v_names))\n",
    "dict_d2v = dict(zip(d2v_names, d2v_vals))\n",
    "\n",
    "\n",
    "\n",
    "#print(dict_d2v)\n",
    "#print(rev_dict_d2v)\n",
    "print(len(d2v_names), len(d2v_vals), len(dict_d2v), len(rev_dict_d2v))\n",
    "\n",
    "testSet = pd.read_csv(\"data/test_data_wIndex.txt\", sep=\",\", header=0, index_col=0)\n",
    "testSet[\"oids\"] = testSet.strOID.str.split()\n",
    "trainSet = pd.read_csv(\"data/train_data_wIndex.txt\", sep=\",\", header=0, index_col=0)\n",
    "trainSet[\"oids\"] = trainSet.strOID.str.split()\n",
    "\n",
    "trainTimeWeight = pd.read_csv(\"data/serialLogDays.txt\", sep=\",\", header=0, index_col=0, converters={\"logDays\": literal_eval})\n",
    "print(trainTimeWeight.head())\n",
    "#trainTimeWeight[\"weights\"] = trainTimeWeight.logDays.str.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3150.000000\n",
       "mean        0.170962\n",
       "std         0.036520\n",
       "min         0.138099\n",
       "25%         0.144792\n",
       "50%         0.159286\n",
       "75%         0.182920\n",
       "max         0.324862\n",
       "Name: novelty_date, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novelty_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr_objects_similarity(i, j, rev_dict):\n",
    "    try:\n",
    "        idi = cbDict[rev_dict[i]]\n",
    "        idj = cbDict[rev_dict[j]]\n",
    "        return dfCBSim[idi, idj]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def mmr_sorted(docs, lambda_, results, rev_dict, length):\n",
    "    \"\"\"Sort a list of docs by Maximal marginal relevance\n",
    "\n",
    "\tPerforms maximal marginal relevance sorting on a set of\n",
    "\tdocuments as described by Carbonell and Goldstein (1998)\n",
    "\tin their paper \"The Use of MMR, Diversity-Based Reranking\n",
    "\tfor Reordering Documents and Producing Summaries\"\n",
    "\n",
    "    :param docs: a set of documents to be ranked\n",
    "\t\t\t\t  by maximal marginal relevance\n",
    "    :param q: query to which the documents are results\n",
    "    :param lambda_: lambda parameter, a float between 0 and 1\n",
    "    :return: a (document, mmr score) ordered dictionary of the docs\n",
    "\t\t\tgiven in the first argument, ordered my MMR\n",
    "    \"\"\"\n",
    "    #print(\"enter to MMR\")\n",
    "    selected = OrderedDict()\n",
    "    docs = set(docs)\n",
    "    while (len(selected) < len(docs)) and (len(selected) < length):\n",
    "        remaining = docs - set(selected)\n",
    "        mmr_score = lambda x: lambda_ * results[x] - (1 - lambda_) * max(\n",
    "            [mmr_objects_similarity(x, y, rev_dict) for y in set(selected) - {x}] or [0])\n",
    "        next_selected = argmax(remaining, mmr_score)\n",
    "        selected[next_selected] = len(selected)\n",
    "        #print(len(selected))\n",
    "    return selected\n",
    "\n",
    "\n",
    "def argmax(keys, f):\n",
    "    return max(keys, key=f)\n",
    "\n",
    "def user_novelty_at_n(rankedIDs, trainModelIDs, n):\n",
    "    return np.sum([1 for i in rankedIDs[0:n] if i in trainModelIDs])/n\n",
    "\n",
    "def prec_at_n(rankedRelevance, n):\n",
    "    return np.sum(rankedRelevance[0:n])/n\n",
    "\n",
    "def meanNovelty_at_n(noveltyList, n):\n",
    "    return np.sum(noveltyList[0:n])/n\n",
    "\n",
    "def rec_at_n(rankedRelevance, n):\n",
    "    return np.sum(rankedRelevance[0:n])/np.sum(rankedRelevance)\n",
    "\n",
    "def ild_at_n(idx, rev_dict,  n):\n",
    "    divList = []\n",
    "    for i in idx[0:n]:\n",
    "        for j in idx[0:n]:\n",
    "            try:\n",
    "                idi = cbDict[rev_dict[i]]\n",
    "                idj = cbDict[rev_dict[j]]\n",
    "                if i != j:\n",
    "                    divList.append(1-dfCBSim[idi, idj])\n",
    "            except:\n",
    "                pass\n",
    "    return np.mean(divList)\n",
    "\n",
    "def evalResults(results, trueRelevance, noveltyList, trainModelIDs, rev_dict, uid, alg, params, rec, outFile, diversity, novelty):\n",
    "    params = [str(i) for i in params]\n",
    "    #calculate rating precision\n",
    "    mmScaler = MinMaxScaler(copy=True)\n",
    "    results = mmScaler.fit_transform(results.reshape(-1,1))\n",
    "    results = results.reshape((-1,))\n",
    "    r2Sc = r2_score(trueRelevance, results)\n",
    "    mae = mean_absolute_error(trueRelevance, results)\n",
    "\n",
    "\n",
    "    #calculate ranking scores\n",
    "    idx = (-results).argsort()\n",
    "\n",
    "    if diversity == \"yes\":\n",
    "        reranked = mmr_sorted(range(len(results)), 0.8, results, rev_dict, 10)\n",
    "        idx1 = [k for k, v in reranked.items()]\n",
    "        idx2 = [i for i in idx if i not in idx1]\n",
    "        idx1.extend(idx2)\n",
    "        idx = idx1\n",
    "\n",
    "    rankedRelevance = trueRelevance[idx]\n",
    "    rankedNovelty = noveltyList[idx]\n",
    "\n",
    "    #print(rankedRelevance)\n",
    "    idx_rev = list(map(rev_dict.get, idx))\n",
    "    mapr = rank_metrics.average_precision(rankedRelevance)\n",
    "    aucSc = roc_auc_score(trueRelevance, results)\n",
    "    nDCG10 = rank_metrics.ndcg_at_k(rankedRelevance,10)\n",
    "    nDCG100 = rank_metrics.ndcg_at_k(rankedRelevance, 100)\n",
    "    nDCG = rank_metrics.ndcg_at_k(rankedRelevance, len(rankedRelevance))\n",
    "\n",
    "    p5 = prec_at_n(rankedRelevance, 5)\n",
    "    r5 = rec_at_n(rankedRelevance, 5)\n",
    "    n5 = meanNovelty_at_n(rankedNovelty, 5)\n",
    "    un5 = user_novelty_at_n(idx_rev, trainModelIDs, 5)\n",
    "    ild5 = ild_at_n(idx, rev_dict, 5)\n",
    "    p10 = prec_at_n(rankedRelevance, 10)\n",
    "    r10 = rec_at_n(rankedRelevance, 10)\n",
    "    n10 = meanNovelty_at_n(rankedNovelty, 10)\n",
    "    ild10 = ild_at_n(idx, rev_dict, 10)\n",
    "    un10 = user_novelty_at_n(idx_rev, trainModelIDs, 10)\n",
    "\n",
    "    mrr = rank_metrics.mean_reciprocal_rank([rankedRelevance])\n",
    "\n",
    "\n",
    "    #print((uid, alg, \",\".join(params), rec, r2Sc, mae, map, aucSc, mrr, p5, p10, r5, r10, nDCG10, nDCG100, nDCG))\n",
    "\n",
    "    txt = \"%s;%s;%s;%s;%s;%s;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f;%.6f\\n\"%(uid, alg, \",\".join(params), rec, diversity, novelty, r2Sc, mae, mapr, aucSc, mrr, p5, p10, r5, r10, nDCG10, nDCG100, nDCG, n5, n10, un5, un10, ild5, ild10)\n",
    "    outFile.write(txt)\n",
    "    return(r2Sc, mae, mapr, aucSc, mrr, p5, p10, r5, r10, nDCG10, nDCG100, nDCG, n5, n10, ild5, ild10)\n",
    "\n",
    "\n",
    "\n",
    "def eval(model, dictionary, rev_dict, testSet, trainSet, alg, params, resultsFile):\n",
    "    recsysStrategies = [\"temporal\",  \"temporal3\", \"temporal5\", \"temporal10\", \"mean\", \"max\", \"last\", \"window3\", \"window5\", \"window10\"] #, \"diversity\", \"novelty\"\n",
    "\n",
    "    # remove objects that are no longer valid\n",
    "    resultsValidity = [i for i in range(len(rev_dict)) if (rev_dict[i] in dfValidDates.index) and (dfValidDates.available_date[rev_dict[i]] > \"2018-06-01\")]\n",
    "    #print(resultsValidity)\n",
    "\n",
    "    for rec in recsysStrategies:\n",
    "        for uid in testSet.index:\n",
    "            # print(uid)\n",
    "            # print(dictionary)\n",
    "            # print(rev_dict)\n",
    "            # exit()\n",
    "            try:\n",
    "                userTrainData = [int(i) for i in trainSet.oids[uid]]\n",
    "                userTestData = [int(i) for i in testSet.oids[uid]]\n",
    "\n",
    "                # remove no longer known IDs\n",
    "                trainModelIDs = list(map(dictionary.get, userTrainData))\n",
    "                if (rec == \"temporal\") |(rec == \"temporal3\") |(rec == \"temporal5\") |(rec == \"temporal10\"):\n",
    "                    tw = trainTimeWeight.logDays[uid]\n",
    "                    tw = [tw[i] for i in range(len(tw)) if trainModelIDs[i] is not None]\n",
    "\n",
    "                trainModelIDs = list(filter(None.__ne__, trainModelIDs))\n",
    "                userTrainData = list(map(rev_dict.get, trainModelIDs))\n",
    "\n",
    "                testModelIDs = list(map(dictionary.get, userTestData))\n",
    "                testModelIDs = list(filter(None.__ne__, testModelIDs))\n",
    "                userTestData = list(map(rev_dict.get, testModelIDs))\n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"Error for user \" + str(uid))\n",
    "                userTrainData = []\n",
    "                userTestData = []\n",
    "            # print(len(userTrainData), len(userTestData))\n",
    "            if (len(userTrainData) > 0) & (len(userTestData) > 0):\n",
    "\n",
    "                trueRelevance = np.zeros(len(dictionary.keys()), dtype=int)\n",
    "                trueRelevance[testModelIDs] = 1\n",
    "                allTrainModelIDs = trainModelIDs\n",
    "\n",
    "                if (rec == \"mean\") | (rec == \"max\"):\n",
    "                    weights = [1.0] * len(userTrainData)\n",
    "                elif rec == \"last\":\n",
    "                    userTrainData = userTrainData[-1:]\n",
    "                    trainModelIDs = trainModelIDs[-1:]\n",
    "                    weights = [1.0]\n",
    "                elif rec == \"window3\":\n",
    "                    userTrainData = userTrainData[-3:]\n",
    "                    trainModelIDs = trainModelIDs[-3:]\n",
    "                    weights = [1 / len(userTrainData) * i for i in range(1, (len(userTrainData) + 1))]\n",
    "                elif rec == \"window5\":\n",
    "                    userTrainData = userTrainData[-5:]\n",
    "                    trainModelIDs = trainModelIDs[-5:]\n",
    "                    weights = [1 / len(userTrainData) * i for i in range(1, (len(userTrainData) + 1))]\n",
    "                elif rec == \"window10\":\n",
    "                    userTrainData = userTrainData[-10:]\n",
    "                    trainModelIDs = trainModelIDs[-10:]\n",
    "                    weights = [1 / len(userTrainData) * i for i in range(1, (len(userTrainData) + 1))]\n",
    "\n",
    "                elif rec == \"temporal3\":\n",
    "                    #userTrainData = userTrainData[-3:]\n",
    "                    trainModelIDs = trainModelIDs[-3:]\n",
    "                    weights = [float(i) for i in tw[-3:]]\n",
    "                    #weights = [1 / len(userTrainData) * i for i in range(1, (len(userTrainData) + 1))]\n",
    "                elif rec == \"temporal5\":\n",
    "                    #userTrainData = userTrainData[-5:]\n",
    "                    trainModelIDs = trainModelIDs[-5:]\n",
    "                    weights = [float(i) for i in tw[-5:]]\n",
    "                    #weights = [1 / len(userTrainData) * i for i in range(1, (len(userTrainData) + 1))]\n",
    "                elif rec == \"temporal10\":\n",
    "                    #userTrainData = userTrainData[-10:]\n",
    "                    trainModelIDs = trainModelIDs[-10:]\n",
    "                    weights = [float(i) for i in tw[-10:]]\n",
    "                    #weights = [1 / len(userTrainData) * i for i in range(1, (len(userTrainData) + 1))]\n",
    "                elif rec == \"temporal\":\n",
    "                    weights = [float(i) for i in tw]\n",
    "                    #weights = [1 / len(userTrainData) * i for i in range(1, (len(userTrainData) + 1))]\n",
    "\n",
    "\n",
    "                # print(trainModelIDs)\n",
    "                # print(type(trainModelIDs[0]))\n",
    "                embeds = model[trainModelIDs]\n",
    "                # print(embeds.shape)\n",
    "                if alg == \"attributeCosineSim\":\n",
    "                    results = embeds\n",
    "                else:\n",
    "                    results = 1 - pairwise_distances(embeds, model, metric=\"cosine\")\n",
    "\n",
    "                weights = np.asarray(weights).reshape((-1, 1))\n",
    "                if rec == \"max\":\n",
    "                    results = np.max(results, axis=0)\n",
    "                else:\n",
    "                    results = results * weights\n",
    "                    results = np.mean(results, axis=0)\n",
    "                # print(results.shape, np.sum(trueRelevance))\n",
    "                results = results[resultsValidity]\n",
    "                trueRelevance = trueRelevance[resultsValidity]\n",
    "\n",
    "                noveltyList = np.asarray(list(map(noveltyDict.get, [rev_dict[i] for i in range(len(rev_dict))])))\n",
    "                noveltyList = noveltyList[resultsValidity]\n",
    "                \n",
    "                allTrainModelIDs = [rev_dict[i] for i in allTrainModelIDs if i in resultsValidity]\n",
    "                trainModelIDs = [i for i in trainModelIDs if i in resultsValidity]\n",
    "\n",
    "                rdKeys = range(len(results))\n",
    "                rdVals = [rev_dict[i] for i in resultsValidity]\n",
    "                rev_dict_updated = dict(zip(rdKeys, rdVals))\n",
    "\n",
    "                if (np.sum(trueRelevance) > 0):\n",
    "                    resultMetrics = evalResults(results, trueRelevance, noveltyList, allTrainModelIDs, rev_dict_updated, uid, alg, params, rec, resultsFile, \"no\", \"no\")\n",
    "                    resultMetrics = evalResults(results, trueRelevance, noveltyList, allTrainModelIDs, rev_dict_updated, uid, alg, params, rec, resultsFile, \"yes\", \"no\")\n",
    "                    # enhance novelty as a (1 + nov_score) re-ranking to the results list\n",
    "                    results = (0.8* results)  + (0.2*noveltyList)\n",
    "                    resultMetrics = evalResults(results, trueRelevance, noveltyList, allTrainModelIDs, rev_dict_updated, uid, alg, params, rec, resultsFile,\"no\",\"yes\")\n",
    "                    resultMetrics = evalResults(results, trueRelevance, noveltyList, allTrainModelIDs, rev_dict_updated, uid, alg, params, rec, resultsFile, \"yes\",\"yes\")\n",
    "\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open('obj/' + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval CB\n",
      "eval CB\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval D2V\n",
      "eval W2V\n",
      "eval W2V\n",
      "eval W2V\n",
      "eval W2V\n",
      "eval W2V\n",
      "eval W2V\n",
      "eval W2V\n",
      "eval W2V\n",
      "eval W2V\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "with open(\"resultsWithNovDiv_32_0dot01Temporal.csv\",\"w\") as resultsFile:\n",
    "    resultsFile.write(\"uid;alg;params;recAlg;noveltyEnhance;diversityEnhance;r2Score;mae;map;aucScore;mrr;p5;p10;r5;r10;nDCG10;nDCG100;nDCGFull;novelty5;novelty10;user_novelty5;user_novelty10;ild5;ild10\\n\")\n",
    "    for alg in listOfAlgs:\n",
    "        if alg == \"word2vec\":\n",
    "            for e in embedSizes:\n",
    "                for w in windowSizes:\n",
    "                    #model, rev_dict, dictionary = word2vec.word2vecRun(w,e)\n",
    "                    #dictionary = dict([((int(i),j) if i !=\"RARE\" else (-1,j)) for i,j in dictionary.items() ])\n",
    "                    #rev_dict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "                    #store models\n",
    "\n",
    "                    model = load_obj(\"word2vec_{0}_{1}_model\".format(e,w))\n",
    "                    dictionary = load_obj(\"word2vec_{0}_{1}_dict\".format(e, w))\n",
    "                    rev_dict = load_obj(\"word2vec_{0}_{1}_revdict\".format(e, w))\n",
    "\n",
    "                    print(\"eval W2V\")\n",
    "                    eval(model, dictionary, rev_dict, testSet, trainSet, alg, (e, w), resultsFile)\n",
    "        elif alg == \"doc2vec\":\n",
    "            for e in embedSizes:\n",
    "                for w in windowSizes:\n",
    "                    #model = doc2vec.doc2vecRun(w,e)\n",
    "                    #rev_dict = rev_dict_d2v\n",
    "                    #dictionary = dict_d2v\n",
    "                    # store models\n",
    "\n",
    "                    model = load_obj(\"doc2vec_{0}_{1}_model\".format(e, w))\n",
    "                    dictionary = load_obj(\"doc2vec_dict\")\n",
    "                    rev_dict = load_obj(\"doc2vec_revdict\")\n",
    "\n",
    "                    print(\"eval D2V\")\n",
    "                    eval(model, dictionary, rev_dict, testSet, trainSet, alg, (e,w), resultsFile)\n",
    "        else:\n",
    "            #TODO get CB data\n",
    "\n",
    "            #rev_dict = rev_cbDict\n",
    "            #dictionary = cbDict\n",
    "\n",
    "            dictionary = load_obj(\"vsm_dict\")\n",
    "            rev_dict = load_obj(\"vsm_revdict\")\n",
    "\n",
    "            for same in [\"sameAllowed\", \"noSameObjects\"]:\n",
    "                if same == \"sameAllowed\":\n",
    "                    #model = dfCBSim\n",
    "                    model = load_obj(\"vsm_{0}_model\".format(same))\n",
    "                else:\n",
    "                    #model = dfCBSimNoSame\n",
    "                    model = load_obj(\"vsm_{0}_model\".format(same))\n",
    "                print(\"eval CB\")\n",
    "                eval(model, dictionary, rev_dict, testSet, trainSet, alg, [same], resultsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
